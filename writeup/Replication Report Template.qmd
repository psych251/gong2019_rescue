---
title: "Replication of 'Are Older Adults More Willing to Donate? The Roles of Donation Form and Social Relationship' by Gong, Zhang & Fung (2019, The Journals of Gerontology: Series B)"
author: "Raphael Uricher (Email: ruricher@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
format:
  html:
    toc: true
    toc_depth: 3
output: github_document
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

My research interests lie broadly in culture, emotion, and well-being, topics that may not appear to be directly related to this study at first glance. However, the complex interplay between age, donation behaviour, and various relational factors (i.e., kinship, social distance) described in the original paper intuitively appears to me to be shaped by culture. Culture moulds how we understand age, our attitudes towards donation, and -- perhaps most importantly here -- how we relate to our families and close others (vs our non-relatives and strangers). When I looked over the original paper, I was interested to see that the sample was composed of "community-dwelling Chinese adults" apparently living in Hong Kong and immediately wondered if possible cultural differences between the sample used in original study and the sample used in the first replication attempt may have been implicated in the failure to replicate. Indeed, the authors of the original paper suggest the possibility of culture influencing their results, writing that "future studies should test whether this age difference in kin-selection in prosocial behaviors varies across cultures". If possible within the constraints of this project, I would be interested in potentially controlling for cultural background in my replication attempt, but I am very interested in this project in any case.

To conduct this experiment, I will need to collect data from a sample of older and younger adults. I will need to create a survey containing the following demographic items: gender, age, core social network size (i.e., estimated number of important relatives/friends), level of education, and household income. Older participants will also be asked to complete the Alzheimer’s Disease Assessment Scale-Cognitive Subscale (ADAS-Cog) as a measure of cognitive function. All participants will be asked to complete a hypothetical donation task in they will be asked how much money or time they would donate to a number of targets who systematically vary in their closeness to and kinship with the participant. The study will use a mixed design in which age (older vs younger) is a between-subjects variable and donation form (money vs time), kinship with the target (relative vs non-relative), and social distance with the target (targets ranked 1st, 2nd, 5th, 10th, 20th, 50th, and 100th in a list of 100 people ranked in descending order of closeness) are within-subjects variables.

Analysis will involve first fitting a hyperbolic discount function to group data and testing for goodness of fit, then fitting a hyperbolic discount function to individual data to find each participant's discount rate (k) and area under curve (AUC) values. k and AUC values will then be log-transformed. Correlations between demographic variables and transformed k and AUC values will be investigated, and finally I will run a mixed 2 (age) × 2 (donation form) × 2 (kinship) repeated measures ANOVA on transformed k and AUC values.

A key challenge will be recruitment of a sample of older adults. The older adult sample in the original study were 60-84 years old and it may be difficult to recruit people in this age bracket through Prolific. It may be worthwhile considering a) not including the older adult sample, b) the benefits and pitfalls of recruiting a slightly younger older adult sample, c) whether it might be possible to include age as a continuous rather than categorical variable and changing the analysis strategy accordingly, or d) including age as a control variable rather than as an independent variable of primary interest. A personal challenge will be learning how to fit a function to data; this is something I've never done before but a skill I'm looking forward to developing.

[Github Repository](https://github.com/psych251/gong2019_rescue)

[Original Paper](https://github.com/psych251/gong2019_rescue/blob/main/original_paper/gbx099.pdf)

[Link to Qualtrics survey](https://stanforduniversity.qualtrics.com/jfe/form/SV_4Trmek4bc8vVDp4)

## Summary of prior replication attempt

The prior replication attempt was conducted by Yochai Shavit in 2017 and was unable to replicate Gong et al.'s (2019) main findings. However,  Shavit's replication attempt departed from the original study on a number of dimensions. Firstly, the original sample consisted of 89 younger (age range: 18-44, mean: 30.1) and 66 older adults (age range: 60-84, mean: 69.4) from Hong Kong. In the replication attempt, based on a power analysis indicating that data from 45 younger and 45 older adults would be sufficient to detect a main effect of age with 80% power in a one-tailed t-test, 43 younger (age range: 19-35, mean: 28.2) and 47 older (age range: 57-74, mean: 62.7) adults living in the United States were recruited. The samples thus differed in three key ways: the sample in the replication study was smaller, the participants in the replication study were younger, and the samples represented two very different cultures. Of these, Shavit suggests - and I concur - that the factor likely to be most implicated in the failure to replicate is culture, given there are different norms surrounding donation and attitudes towards relatives in Hong Kong and the United States. Additional differences in implementation and analysis were as follows:

1) The original study was conducted in person in a lab, whereas the replication study was conducted online. Given the study's variables essentially consist exclusively of a survey, this seems unlikely to have caused any issues.

2) Participants in the original study were asked to imagine they had 100,000 Hong Kong dollars (HKD), whereas participants in the replication study were asked to imagine they had 100,000 US dollars (USD) to fit the context in which the data was collected. 100,000 HKD is approximately equivalent to 13,000 USD. Shavit consulted the first author of the original study about how to reconcile this discrepancy and was told to instruct participants to imagine having 100,000 USD rather than an amount equivalent to 100,000 HKD. Because this study focuses on how much participants are relatively willing to donate to others as a function of how close they are and whether they are related to them, rather than the absolute amount they are willing to donate, I feel there should be minimal issues with this approach.

3) In the original study, older participants' cognitive function was assessed using the cognitive subscale of the Alzheimers' Disease Assessment Scale (ADAS-Cog) to ensure the results would not be confounded with cognitive function. Because this is not a computer-based test, it was not used in the replication study. This seems unlikely to have been an issue because, in the original study, cognitive function and willingness to donate were not significantly correlated.

4) Because the replication study was conducted online, Shavit included a simple attention check and excluded participants who failed it.

5) The authors of the original study fit the hyperbolic discount function to the data using Matlab, but Shavit did this using the “optim” function from the ‘stats’ package in R. Shavit was able to obtain similar results to the original authors when testing this procedure a priori using simulated data, suggesting that differences in the results were not an artefact of different software being used for analysis.

6) The authors of the original study did not report how they dealt with missing data. In the replication study, Shavit excluded participants with missing AUC values from analysis.

7) In the original study, the younger adult group was comprised of participants aged between 18 and 44. Shavit elected to collect data from participants aged between 18 and 35. Additionally, the youngest participant/s in the older adult group in the original study was/were 60, but the youngest participant/s in the older adult group in the replication study was/were 57 because of constraints with regard to participant screening on Mechanical Turk.

8) The authors of the original study did not report using any post-hoc correction when conducting t-tests. In the replication study, Shavit reported the results of t-tests both with and without Bonferroni correction.


## Methods

### Power Analysis

It was not possible to conduct one single, definitive power analysis for either the original study or the replication study because GPower requires *the correlation between the repeated measures* as an input parameter for post-hoc power analyses of ANOVA which include repeated measures and between-subjects factors, and neither the original paper nor the write-up of the replication attempt reported this.

However, for the original study (which had partial eta-squared of 0.05, a total sample size of 155, 2 groups and 4 measurements), observed power would have been at least 0.81 (if the correlation between the measurements was 0.99) and up to over 0.99 (if the correlation between between the measurements was 0.01). It is thus safe to say the original study was sufficiently powered.

In the first replication attempt (which had partial eta-squared of 0.025, a total sample size of 90, and as before, 2 groups and 4 measurements), observed power could have been anywhere between 0.33 (if the correlation between the measurements was 0.99) and 0.84 (if the correlation between between the measurements was 0.01). It was thus likely underpowered.

### Planned Sample

Based on the power analyses described above and budgetary constraints, I will collect data from 156 participants (78 for each group), which approximately corresponds to the original study's sample size.

I will collect data from both a younger sample and an older sample. The author of the replication study suggested that cultural differences may have driven the failure to replicate. I will not be able to collect data specifically from Hong Kong Chinese participants as in the original study due to a dearth of Hong Kong residents actively using Prolific, but I will recruit participants who report their ethnicity as East Asian to try to sample people with more similar cultural backgrounds to the participants in the original study than those who participated in the first replication attempt.

In the original study, younger participants were aged between 18 and 44, and older participants were aged over 60. However, at the time of preparing this report, there were only 30 East Asian workers who were active on Prolific in the past 30 days. This number increased to 73 when minimum age was set to 55 (the lower end of the age distribution for the older group in the first replication attempt), but given this is still smaller than the sample size I aim to collect and it is unlikely all of these workers would participate in the study in any case, I will allow participants as young as 50 to participate as part of the older group. I believe this is justifiable because it seems most plausible that cultural differences drove the failure to replicate in the first replication attempt, so it is arguably pertinent to prioritise recruiting participants of East Asian background rather than participants aged over 55/60 for this rescue project.

### Materials

Participants will be asked to list their gender, age, level of education, and household income. As a measure of core social network size, participants will be asked to estimate the number of important friends and family they have in their lives. 

Participants will then be asked to complete hypothetical donation tasks for both relatives and non-relatives. The initial instruction will be as follows: “The following task asks you to imagine that you have a list of 100 relatives (non-relatives) arranged in descending order of their closeness with you. The person at position #1 is your closest relative (non-relative), while the person at position #100 is a relative (non-relative) you may know but are not close at all. You do not have to actually create the list – just imagine that you have done so. Next you will be asked to answer questions regarding these relatives (non-relatives) at a given position.”

Participants will then be asked 14 questions regarding how much money and time they would be willing to donate to the relatives and non-relatives at each of the following positions on those lists: 1st, 2nd, 5th, 10th, 20th, 50th, and 100th. The questions will read as follows: “Imagine that the relative (non-relative) at position N on your list is now hospitalized. He/she needs to be accompanied and taken care of in the hospital, as well as a large amount of money to pay for the medical expense. If you now own 100 thousand of US dollars and 100 days of vacation, how much money would you donate to him/her, and how many days would you spend taking care of him/her?”

The only difference between the materials described here and those used in the original study is that participants will be asked to imagine they have 100,000 US dollars rather than 10,000 HK dollars, following the instructions given to the author of the first replication attempt from the first author of the original study.

### Procedure	

Participants will be asked to complete a Qualtrics survey containing the measures described above in the order they appear above.

### Controls

Following the first replication attempt, I will include a simple attention check. After participants are asked how much time and money they would be willing to donate to a relative at position 20 on their list, they will also be asked the following question: “In the last question, your answers referred to the participant in position:” with 3, 7, 12, 16, 20, 28, and 42 as answer choices. Participants who do not answer '20' will be excluded from analysis.

### Analysis Plan

Firstly, identifiable information will be removed from the data set. Next, I will follow the analysis plan outlined in the original study:

>To analyze the donation data, we first modeled the group donation data. For each age × donation form × kinship cell, a hyperbolic discount function (i.e., Equation 1) was fit to the group median values (Jones & Rachlin, 2006; Rachlin & Jones, 2007, 2008) over the seven social distances (N = 1, 2, 5, 10, 20, 50, and 100): $$v=V/(1+kN)$$ where V is the undiscounted reward value, v is the reward value that a person would like to forgo for the benefit for someone at a social distance N, and k is a constant indexing degree of social discounting (i.e., discount rate) across social distances. Following the convention of the literature, group median values instead of mean values were used because such data were usually not normally distributed (e.g., Jones & Rachlin, 2006).

>The fitting process was conducted by curve fit toolbox of Matlab (version 2015a), where R-square and root-mean-square error (RMSE) were used to assess the goodness of fit. Such a modeling technique has been widely used in the literature on temporal (e.g., Frederick, Loewenstein, & O’Donoghue, 2002) and social discounting (e.g., Jones & Rachlin, 2006). The technique enables us to sketch individuals’ donation willingness across various social distances with simple parameters like discount rate (k) and area under curve (AUC) (e.g., Jones & Rachlin, 2006; Margittai et al., 2015). Discount rate (k) reflects a donor’s social-distance dependent selectivity as to donation willingness—a larger k value indicates a higher level of selectivity, that is, the donor treats socially close and distant recipients more differently. AUC could be used to index the donor’s overall generosity to people independent of social distance, with a larger value indicating a higher level of generosity (Margittai et al., 2015; Strombach et al., 2014).

>Once the appropriateness of the hyperbolic discount function was confirmed for group data, we then conducted the same fitting process to individual data to get the discount rate (k) and the AUC for each participant under different conditions. Again following the convention of the literature, individual data that did not fit the hyperbolic discount function were excluded from further analyses (Vuchinich & Simpson, 1998). The individuals’ k and AUC values were then normalized by a natural logarithm transformation (e.g., Margittai et al., 2015), and transformed values that exceeded ±3 SDs were treated as outliers.

>Next, the transformed individual k and AUC values were correlated with demographic variables to preliminarily check the influences of demographic variables on donation willingness. To examine the moderating roles of donation form and kinship in the relation between age and donation willingness, the transformed individual AUC values and k values were submitted respectively to a 2 (age: younger and older) × 2 (donation form: money and time) × 2 (kinship: relatives and nonrelatives) mixed design, repeated measures ANOVA (and post hoc tests), with and without controlling for demographic variables.

I will depart from the above analysis plan only in that I will use R rather than Matlab to conduct curve fitting and analyse data. Additionally, as in the first replication attempt, I will report the results of t-tests both with and without applying Bonferroni correction.

The key analysis of interest is the mixed design, repeated measures ANOVA of transformed AUC values controlling for demographic variables.

### Differences from Original Study and 1st replication

1) Data will be collected on Prolific rather than in person (as in the original study) or on Mechanical Turk (as in the first replication attempt). This is unlikely to make a difference to the results because this is a survey study.

2) The original study's participants were Hong Kong Chinese adults living in Hong Kong, whereas the first replication attempt sampled residents of the United States of any ethnicity. Rather than recruiting participants who live in a specific country, I will recruit participants who identify as Asian and/or list a Chinese language (e.g., Mandarin, Cantonese) as their first language to attempt to sample people with a relatively similar cultural background to the original study's participants. If cultural differences were indeed implicated in the failure to replicate, this strategy may increase the chances of replicating the original study's findings.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Load relevant libraries
```{r}
library(knitr)
library(tidyverse)
library(ggplot2)
library(car)
library(ez)
library(broom)
library(pander)
library(kableExtra)
library(lme4)
library(lmerTest)
library(effsize)
library(RColorBrewer)
```
Write function to compute hyperbolic function
```{r}
discount <- function (V, k, N) {
  nu <- V / (1 + k * N)
  
  return(nu)
}
```

Write function to compute RMSE
```{r}
data_vs_discount <- function(V, k, N, amount) {
  nu <- discount(V, k, N)
  
  RMSE <- sqrt(mean((amount - nu)^2))
  
  return(RMSE)
}
```

Import data
```{r}
df = read.csv("psyc251_pilot_b_data.csv", header=T)
df <- tail(df, -2)
```

Remove identifying information and extraneous variables
```{r}
df = select(df, -1:-17, -59)
```

Add subject IDs
```{r}
df$id <- 1:25
```

Remove participants who failed attention check and TA
```{r}
df = filter(df, attention_check==5)
df = filter(df, gender_5_TEXT!="I AM YOUR TA PLEASE EXCLUDE ME")
```

Group participants based on age (create an 'age group' column where participants are listed as either 'younger' or 'older')
```{r}
df <- df%>%
  mutate(age_group = dplyr::case_when(age <= 39 ~ "younger", age > 40 ~ "older"),
         age_group = factor(age_group, level = c("younger", "older"))
  )

#doing 40+ as older group just for the purposes of piloting but will change this to 60+ (if sampling) or 55+ or 50+ as appropriate (depending on my ability to find older participants on Prolific)

summary(df$age_group)
```

Converting critical variables to numeric
```{r}
df[,c(1:38)]=as.numeric(unlist(df[,c(1:38)]))
```

Renaming money and time preference variables so they don't get caught up in data wrangling
```{r}
df <- df %>%
  mutate(easeM = ease_money, easeT = ease_time, MorT = money_or_time) %>%
  select(-ease_money, -ease_time, -money_or_time)
```

Making data set longer
```{r}
og_lng=df%>%
  gather(condition, amount, c(matches("_money"),matches("_time"))
         )%>%
  separate(condition, c("kin", "soc_dist", "donation"), "_")%>%
  spread(kin, amount)%>%
  gather(kinship, amount,
         rel, nonrel)
```

For each age × donation form × kinship cell, fit hyperbolic discount function (i.e., Equation 1) to group median values over seven social distances (N = 1, 2, 5, 10, 20, 50, and 100)
```{r}
#create df with monetary amounts divided by 1000 for plotting and analyses (to have the same units for time and money donations)
df_og2=df%>%mutate(rel_1_money=rel_1_money/1000, 
                      rel_2_money=rel_2_money/1000, 
                      rel_5_money=rel_5_money/1000, 
                      rel_10_money=rel_10_money/1000, 
                      rel_20_money=rel_20_money/1000, 
                      rel_50_money=rel_50_money/1000, 
                      rel_100_money=rel_100_money/1000, 
                      nonrel_1_money=nonrel_1_money/1000, 
                      nonrel_2_money=nonrel_2_money/1000, 
                      nonrel_5_money=nonrel_5_money/1000, 
                      nonrel_10_money=nonrel_10_money/1000, 
                      nonrel_20_money=nonrel_20_money/1000, 
                      nonrel_50_money=nonrel_50_money/1000, 
                      nonrel_100_money=nonrel_100_money/1000)

#long-form this dataframe
og_lng2=df_og2%>%
  gather(condition, amount, c(matches("_money"),matches("_time"))
         )%>%
  separate(condition, c("kin", "soc_dist", "donation"), "_")%>%
  spread(kin, amount)%>%
  gather(kinship, amount,
         rel,nonrel)

#make variables numeric
og_lng %>%
  mutate_all(as.numeric)

og_lng2 %>%
  mutate_all(as.numeric)

#create table with original median values
med_table=og_lng%>%group_by(age_group,kinship,donation,soc_dist)%>%summarise(med_amnt=median(amount))

#create table of median values with the transformed monetary amounts
med_table2=og_lng2%>%group_by(age_group,kinship,donation,soc_dist)%>%summarise(med_amnt=median(amount))

#make soc_dist variables numeric
med_table <- mutate(med_table, soc_dist = as.numeric(soc_dist))
med_table2 <- mutate(med_table2, soc_dist = as.numeric(soc_dist))
```

8) Check goodness of fit using R-square and RMSE
```{r}
#define dataset
d=med_table2

#create empty vectors to store values
age_group=vector(mode="character",length=0); donation=vector(mode="character",length=0); kinship=vector(mode="character",length=0); V_values=vector(mode="numeric",length=0); k_values=vector(mode="numeric",length=0); RMSE=vector(mode="numeric",length=0); 
R_sq=vector(mode="numeric",length=0)

#run for each age group, extract values for donation * kinship conditions

for(i in unique(d$age_group)){
  
  d_money_rel=d%>%filter(age_group==i, donation=="money", kinship=="rel") #-> create temp df to get values from
  age_group=c(age_group,i) #-> store the appropriate age group
  donation=c(donation, "money") #-> store the appropriate donation
  kinship=c(kinship, "rel") #-> store the appropriate kinship
  optim_discount_wrapper <- function(x) { #-> define discount wrapper to get V, k, and RMSE values for this                                                      condition
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_money_rel$soc_dist, 
                          amount = d_money_rel$med_amnt))
}

  opt1=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100)) #-> run discount_wrapper
  V_values=c(V_values,opt1$par[1]) #-> store resulting V_value
  k_values=c(k_values, opt1$par[2]) #-> store resulting k_value
  RMSE=c(RMSE, opt1$value) #-> store resulting RMSE value
  d_money_rel$pred=discount(V = opt1$par[1], k = opt1$par[2],N = d_money_rel$soc_dist) #-> store predictions in temp                                                                                          dataset
  cor1=cor.test(d_money_rel$med_amnt, d_money_rel$pred) #-> run cor.test to get correlation of predictions and data
  R_sq=c(R_sq, (cor1$estimate)^2) #-> square the estimate (pearson's R) to get R^2.
  
  
  d_money_nonrel=d%>%filter(age_group==i, donation=="money", kinship=="nonrel") #-> do all of this for the next donation                                                                               * Kinship condition
  age_group=c(age_group,i)
  donation=c(donation, "money")
  kinship=c(kinship, "nonrel")
  optim_discount_wrapper <- function(x) {
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_money_nonrel$soc_dist, 
                          amount = d_money_nonrel$med_amnt))
}

  opt2=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100))
  V_values=c(V_values,opt2$par[1])
  k_values=c(k_values, opt2$par[2])
  RMSE=c(RMSE, opt2$value)
  d_money_nonrel$pred=discount(V = opt2$par[1], k = opt2$par[2],N = d_money_nonrel$soc_dist)
  cor2=cor.test(d_money_nonrel$med_amnt, d_money_nonrel$pred)
  R_sq=c(R_sq, (cor2$estimate)^2)


  d_time_rel=d%>%filter(age_group==i, donation=="time", kinship=="rel")
  age_group=c(age_group,i)
  donation=c(donation, "time")
  kinship=c(kinship, "rel")
  optim_discount_wrapper <- function(x) {
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_time_rel$soc_dist, 
                          amount = d_time_rel$med_amnt))
}

  opt3=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100))
  V_values=c(V_values,opt3$par[1])
  k_values=c(k_values, opt3$par[2])
  RMSE=c(RMSE, opt3$value)
  d_time_rel$pred=discount(V = opt3$par[1], k = opt3$par[2],N = d_time_rel$soc_dist)
  cor3=cor.test(d_time_rel$med_amnt, d_time_rel$pred)
  R_sq=c(R_sq, (cor3$estimate)^2)

  
  d_time_nonrel=d%>%filter(age_group==i, donation=="time", kinship=="nonrel")
  age_group=c(age_group,i)
  donation=c(donation, "time")
  kinship=c(kinship, "nonrel")
  optim_discount_wrapper <- function(x) {
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_time_nonrel$soc_dist, 
                          amount = d_time_nonrel$med_amnt))
}

  opt4=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100))
  V_values=c(V_values,opt4$par[1])
  k_values=c(k_values, opt4$par[2])
  RMSE=c(RMSE, opt4$value)
  d_time_nonrel$pred=discount(V = opt4$par[1], k = opt4$par[2],N = d_time_nonrel$soc_dist)
  cor4=cor.test(d_time_nonrel$med_amnt, d_time_nonrel$pred)
  R_sq=c(R_sq, (cor4$estimate)^2)

}

#Add all vectors to one dataset
med_smry=as.data.frame(cbind(`Age group`=age_group, Donation=donation, Kinship=kinship, V=V_values, k=k_values, `R sqr`=R_sq, RMSE), row.names = F)%>%mutate(V=as.character(V), k=as.character(k), `R sqr`=as.character(`R sqr`), RMSE=as.character(RMSE))%>%mutate(V=as.numeric(V), k=as.numeric(k), `R sqr`=as.numeric(`R sqr`), RMSE=as.numeric(RMSE))

kable(med_smry, digits = 2, align = "c", caption = "Estimated Parameters and Goodness-of-fit Indices of the Hyperbolic Functions, group median values")
```

9) Fit hyperbolic discount function to individual data and calculate k and AUC values for each participant in each condition
```{r}
#Create the empty dataframe, in which each participant have 4 rows
df2=as.data.frame(cbind(
  id=rep(unique(og_lng2$id),4),
  age_group=c(),
  gender=c(),
  education=c(),
  income=c(),
  important_friends=c(),
  important_family=c(),
  ethnicity=c(),
  kinship=c(),
  donation=c(),
  k=c(),
  AUC=c()
))

#Fill in values for demographics (age, age group, gender, education, income, number of important family and friends)
for (i in unique(df2$id)){
  df_sub=filter(df_og2, id==i)
  df2[df2$id==i, 'age']=df_sub$age
  df2[df2$id==i, 'age_group']=df_sub$age_group
  df2[df2$id==i, 'gender']=df_sub$gender
  df2[df2$id==i, 'education']=df_sub$education
  df2[df2$id==i, 'income']=df_sub$income
  df2[df2$id==i, 'important_friends']=df_sub$important_friends
  df2[df2$id==i, 'important_family']=df_sub$important_family
  df2[df2$id==i, 'ethnicity']=df_sub$ethnicity
}
```

```{r}
df2=as.data.frame(cbind(
  id=rep(unique(og_lng2$id),4),
  age_group=c(),
  gender=c(),
  education=c(),
  income=c(),
  important_friends=c(),
  important_family=c(),
  ethnicity=c(),
  kinship=c(),
  donation=c(),
  k=c(),
  AUC=c()
))

#Fill in values for demographics
for (i in unique(df2$id)){
  df_sub=filter(df_og2, id==i)
  df2[df2$id==i, 'age']=df_sub$age
  df2[df2$id==i, 'age_group']=df_sub$age_group
  df2[df2$id==i, 'gender']=df_sub$gender
  df2[df2$id==i, 'education']=df_sub$education
  df2[df2$id==i, 'income']=df_sub$income
  df2[df2$id==i, 'important_friends']=df_sub$important_friends
  df2[df2$id==i, 'important_family']=df_sub$important_family
  df2[df2$id==i, 'ethnicity']=df_sub$ethnicity
}
```

```{r}
#for each individual, extract values for donation * kinship conditions

#set variable soc_dist in og_lng2 to be numeric
og_lng2$soc_dist=as.character(og_lng2$soc_dist)
og_lng2$soc_dist=as.numeric(og_lng2$soc_dist)
is.numeric(og_lng2$soc_dist)

#Get length of number of participants (n) as value
n_p=length(unique(df2$id))

#Create 4 temporary df's to store values from each iteration
df2_mr=df2[1:n_p,] #-> for money_relative
df2_mnr=df2[(n_p+1):(n_p*2),] #->for money_nonrelative
df2_tr=df2[(2*n_p+1):(n_p*3),] #->for time_relative
df2_tnr=df2[(3*n_p+1):(n_p*4),] #->for time_nonrel


#run discount wrapper for each subject in 2*2 conditions
for(i in unique(df2$id)){
  #money relative
  d_mon_rel=og_lng2%>%filter(id==i, donation=="money", kinship=="rel") #-> create temp df to get values from
  df2_mr[df2_mr$id==i, 'donation']="money" #-> store the appropriate donation
  df2_mr[df2_mr$id==i, 'kinship']="rel" #-> store the appropriate kinship
  
  optim_discount_wrapper <- function(x) { #-> define discount wrapper to get V, k, and RMSE values for this                                                      condition
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_mon_rel$soc_dist, 
                          amount = d_mon_rel$amount))
}

  opt1=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100)) #-> run discount_wrapper
  df2_mr[df2_mr$id==i, 'k']=opt1$par[2] #-> store resulting k_value
  df2_mr[df2_mr$id==i, 'V']=opt1$par[1] #-> store resulting V_value

  # Get AUC value - using WolframAlpha, the integral of this discount function is:
# V/k*ln(abs(1+k*n))+c -> we don't really care about c for AUC.
  k=opt1$par[2]
  V=opt1$par[1]

  #Evaluate for upper bound
  up=V/k*log(abs(1+k*100))
  lower=V/k*log(abs(1+k*0))

  df2_mr[df2_mr$id==i,'AUC']=up-lower
  
#money nonrelative
  d_mon_nonrel=og_lng2%>%filter(id==i, donation=="money", kinship=="nonrel") 
  df2_mnr[df2_mnr$id==i, 'donation']="money" 
  df2_mnr[df2_mnr$id==i, 'kinship']="nonrel" 
  
  optim_discount_wrapper <- function(x) { 
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_mon_nonrel$soc_dist, 
                          amount = d_mon_nonrel$amount))
}

  opt2=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100)) 
  df2_mnr[df2_mnr$id==i, 'k']=opt2$par[2] 
  df2_mnr[df2_mnr$id==i, 'V']=opt2$par[1]

  # Get AUC value 
  k=opt2$par[2]
  V=opt2$par[1]

  #Evaluate for upper bound
  up=V/k*log(abs(1+k*100))
  lower=V/k*log(abs(1+k*0))

  df2_mnr[df2_mnr$id==i,'AUC']=up-lower
  
#time relative
  d_time_rel=og_lng2%>%filter(id==i, donation=="time", kinship=="rel")
  df2_tr[df2_tr$id==i, 'donation']="time" 
  df2_tr[df2_tr$id==i, 'kinship']="rel" 
  
  optim_discount_wrapper <- function(x) { 
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_time_rel$soc_dist, 
                          amount = d_time_rel$amount))
}

  opt3=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100)) 
  df2_tr[df2_tr$id==i, 'k']=opt3$par[2]
  df2_tr[df2_tr$id==i, 'V']=opt3$par[1]

  # Get AUC value 
  k=opt3$par[2]
  V=opt3$par[1]

  #Evaluate for upper bound
  up=V/k*log(abs(1+k*100))
  lower=V/k*log(abs(1+k*0))

  df2_tr[df2_tr$id==i,'AUC']=up-lower
  
#time nonrelative
  d_time_nonrel=og_lng2%>%filter(id==i, donation=="time", kinship=="nonrel")
  df2_tnr[df2_tnr$id==i, 'donation']="time" 
  df2_tnr[df2_tnr$id==i, 'kinship']="nonrel" 
  
  optim_discount_wrapper <- function(x) { 
  return(data_vs_discount(V = x[1], k = x[2],  
                          N = d_time_nonrel$soc_dist, 
                          amount = d_time_nonrel$amount))
}

  opt4=optim(c(100, 0.2), optim_discount_wrapper, method = "L-BFGS-B", lower=c(0,0), upper=c(300,100)) 
  df2_tnr[df2_tnr$id==i, 'k']=opt4$par[2]
  df2_tnr[df2_tnr$id==i, 'V']=opt4$par[1]

  # Get AUC value 
  k=opt4$par[2]
  V=opt4$par[1]

  #Evaluate for upper bound
  up=V/k*log(abs(1+k*100))
  lower=V/k*log(abs(1+k*0))

  df2_tnr[df2_tnr$id==i,'AUC']=up-lower
}

#Combine all into one dataframe (df2)
df2=full_join(df2_mr, df2_mnr)%>%full_join(df2_tr)%>%full_join(df2_tnr)
df2_plot_pred=df2
```

10) Exclude participants whose data does not fit hyperbolic discount function
11) Normalise individual k and AUC values via natural log transformation
12) Exclude individuals with transformed values that exceed ±3 SDs
13) Correlational analyses on demographic variables and transformed individual k/AUC values

```{r}
#recode all NaN AUC values as missing data (NA)
for(i in 1:length(df2$id)){
  if(is.nan(df2$AUC[i])){df2$AUC[i]=NA}else{df2$AUC[i]=df2$AUC[i]}
}

#Negative or zero AUC or k values (indicating a problem with the fitting process)
df2$AUC=recode(df2$AUC, "lo:0.01=NA")
df2$k=recode(df2$k, "lo:0.01=NA")

#check if AUC is normally distributed
AUc_hist=ggplot(data=df2, aes(x=AUC))+geom_histogram() #-> strong positive skew
logAUC_hist=ggplot(data = df2, aes(x=log(AUC)))+geom_histogram() #-> looks better

#check if k is normally distributed
k_hist=ggplot(data=df2, aes(x=k))+geom_histogram() #-> strong positive skew
log_k_hist=ggplot(data = df2, aes(x=log(k)))+geom_histogram() #-> looks better

#Since Gong et al., used log(AUC) and log(k) for their analysis, I shall do the same
df2$logAUC=log(df2$AUC)
df2$log_k=log(df2$k)

#compute +/- 3 S.D for logAUC and log_k values
logAUC_hi=mean(df2$logAUC, na.rm=T)+(3*sd(df2$logAUC, na.rm = T))
logAUC_lo=mean(df2$logAUC, na.rm=T)-(3*sd(df2$logAUC, na.rm = T))

log_k_hi=mean(df2$log_k, na.rm=T)+(3*sd(df2$logAUC, na.rm = T))
log_k_lo=mean(df2$log_k, na.rm=T)-(3*sd(df2$logAUC, na.rm = T))

#df2=df2%>%filter(k<=(mean(k)+3*sd(k)) & k>=(mean(k)-3*sd(k)) & AUC<=(mean(AUC)+3*sd(AUC))& AUC>=(mean(AUC)-3*sd(AUC))) #-> retain only observations that meet criteria

#recode all observations outside of the +/- 3 S.D range as missing values
df2$logAUC[df2$logAUC<logAUC_lo]=NA
df2$logAUC[df2$logAUC>logAUC_hi]=NA

df2$log_k[df2$log_k<log_k_lo]=NA
df2$log_k[df2$log_k>log_k_hi]=NA


#check that the minimal values are not lower than k and AUC "_low"'s, and maximal not higher than "_high"'s
min(df2$log_k, na.rm = T)<log_k_lo; max(df2$log_k,na.rm = T)>log_k_hi; min(df2$logAUC,na.rm = T)<logAUC_lo; max(df2$logAUC,na.rm = T)>logAUC_hi
```

```{r}

#Change id to factor
is.numeric(df2$id) #-> true
df2$id=as.character(df2$id); df2$id=as.factor(df2$id)
is.factor(df2$id)#-> true
summary(df2$age_group); summary(df2$donation); summary(df2$kinship) 

# check that predictors are factors and AUC is numeric
is.factor(df2$age_group); is.factor(df2$donation); is.factor(df2$kinship) #-> age_group yes, others no

is.character(df2$donation)#-> true
df2$donation=as.factor(df2$donation); is.factor(df2$donation)
is.character(df2$kinship)#-> true
df2$kinship=as.factor(df2$kinship); is.factor(df2$kinship)

is.numeric(df2$logAUC)#-> True

## Remove all observations of subjects with outliers

na_counts=df2%>%group_by(id)%>%summarise(naAUC_counts=sum(is.na(logAUC)),na_k_counts=sum(is.na(log_k)))%>%ungroup()

for (i in unique(df2$id)){
  temp=filter(na_counts, id==i)
  df2[df2$id==i, 'naAUC_counts']=temp$naAUC_counts
  df2[df2$id==i, 'na_k_counts']=temp$na_k_counts
}

df2_noNA_AUC=df2%>%filter(naAUC_counts==0)
df2_noNA_k=df2%>%filter(na_k_counts==0)
```

### Confirmatory analysis

1) 2 (age: younger vs older) × 2 (donation form: money vs time) × 2 (kinship: relatives vs non-relatives) mixed design, repeated measures ANOVA of transformed individual AUC values, with demographic variables as control + post-hoc tests if applicable
2) 2 (age: younger vs older) × 2 (donation form: money vs time) × 2 (kinship: relatives vs non-relatives) mixed design, repeated measures ANOVA of transformed individual AUC values, without demographic variables as control + post-hoc tests if applicable
3) 2 (age: younger vs older) × 2 (donation form: money vs time) × 2 (kinship: relatives vs non-relatives) mixed design, repeated measures ANOVA of transformed individual k values, with demographic variables as control + post-hoc tests if applicable
4) 2 (age: younger vs older) × 2 (donation form: money vs time) × 2 (kinship: relatives vs non-relatives) mixed design, repeated measures ANOVA of transformed individual k values, without demographic variables as control + post-hoc tests if applicable
```{r}
#preparing the datasets for ANOVA by removing missing values
df2_noNA_AUC <- na.omit(df2_noNA_AUC[, c("id", "logAUC", "important_family", "important_friends", "donation", "kinship", "age_group", "gender")])

df2_noNA_k <- na.omit(df2_noNA_k[, c("id", "log_k", "important_family", "important_friends", "donation", "kinship", "age_group", "gender")])
```

```{r}
#AUC without covariates
anova_AUC1=ezANOVA(data = df2_noNA_AUC, wid=.(id), dv=na.omit(logAUC), within  = .(donation, kinship), between = .(age_group), type = 3, observed = .(age_group))

kable(anova_AUC1$ANOVA)
```

```{r}
#AUC with covariates
anova_AUC2=ezANOVA(data = df2_noNA_AUC, wid=.(id), dv=na.omit(logAUC), within  = .(donation, kinship), between = .(age_group), between_covariates = .(gender, important_family, important_friends) ,type = 3, observed = .(age_group))

kable(anova_AUC2$ANOVA)
```

```{r}
#k value without covariates
anova_k1=ezANOVA(data = df2_noNA_k, wid=.(id), dv=na.omit(log_k), within  = .(donation, kinship), between = .(age_group), type = 3, observed = .(age_group))

kable(anova_k1$ANOVA)
```

```{r}
#AUC with covariates
anova_k2=ezANOVA(data = df2_noNA_k, wid=.(id), dv=na.omit(log_k), within  = .(donation, kinship), between = .(age_group), between_covariates = .(gender, important_family, important_friends) ,type = 3, observed = .(age_group))

kable(anova_k2$ANOVA)
```

*Three-panel graph with original, 1st replication, and your replication is ideal here*

### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

## Mini meta analysis
Combining across the original paper, 1st replication, and 2nd replication, what is the aggregate effect size? 

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.